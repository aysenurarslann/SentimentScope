# -*- coding: utf-8 -*-
"""fine_tuned_bounti_veri_artırmali_metrikler.ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-PVYKNrc52LhC3Dsfny3ikSBFAxjjk4o
"""

!pip install datasets
!pip install nlpaug

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("/content/drive/MyDrive/30_data_every_each_class_labeled_tweets.csv")  # Dosya adını kendine göre değiştir


print(df.head())

label_map = {"pozitif": 0, "negatif": 1, "nötr": 2}  # Bunu ihtiyacına göre değiştir
df["label"] = df["label"].map(label_map)

# Split dataset (20% for test)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["label"])

import random
import nlpaug.augmenter.word as naw
import pandas as pd

# Define Turkish-compatible augmentation techniques
swap_aug = naw.RandomWordAug(action="swap")  # Randomly swap words
del_aug = naw.RandomWordAug(action="delete")  # Randomly delete words

turkish_synonyms = {
    "iyi": ["güzel", "harika", "mükemmel", "kusursuz", "şahane", "fevkalade", "müthiş"],
    "kötü": ["berbat", "fena", "korkunç", "rezalet", "iğrenç", "dehşet", "çirkin"],
    "mutlu": ["neşeli", "sevinçli", "keyifli", "memnun", "şen", "mesut"],
    "üzgün": ["mutsuz", "kederli", "hüzünlü", "mahzun", "elemli", "dertli"],
    "hızlı": ["çabuk", "süratli", "seri", "hızlandırılmış", "aceleci"],
    "yavaş": ["ağır", "sakin", "durağan", "gevşek", "tembel"],
    "büyük": ["devasa", "muazzam", "kocaman", "geniş", "dev", "heybetli"],
    "küçük": ["ufak", "minik", "dar", "ince", "cılız", "küçücük"],
    "akıllı": ["zeki", "bilge", "dahi", "mantıklı", "zihinli", "kurnaz"],
    "aptal": ["salak", "geri zekalı", "akılsız", "beyinsiz", "ahmak", "budala"],
    "kolay": ["basit", "zahmetsiz", "sade", "düz", "rahat", "hafif"],
    "zor": ["zorlu", "çetin", "güç", "zahmetli", "meşakkatli"],
    "güçlü": ["kuvvetli", "sağlam", "dayanıklı", "kudretli", "metin"],
    "zayıf": ["güçsüz", "halsiz", "cılız", "narin", "nahif", "çelimsiz"],
    "güzel": ["hoş", "harika", "şahane", "nadir", "mükemmel", "sevimli"],
    "çirkin": ["kötü", "iğrenç", "berbat", "değersiz", "rezil"],
    "sevgi": ["aşk", "şefkat", "ilgi", "muhabbet", "hoşgörü"],
    "nefret": ["kin", "öfke", "gazap", "tiksinti", "düşmanlık"],
    "cesur": ["yiğit", "gözü pek", "kahraman", "cengaver", "girişken"],
    "korkak": ["ödlek", "çekingen", "ürkek", "silik", "pısırık"],
    "hassas": ["duyarlı", "ince", "nazik", "dikkatli", "duygusal"],
    "sert": ["katı", "haşin", "kaba", "şiddetli", "keskin"],
    "temiz": ["pak", "arınmış", "düzenli", "hijyenik", "lekesiz"],
    "kirli": ["pis", "lekeli", "dağınık", "tozlu", "çöplük"],
    "soğuk": ["serin", "buz gibi", "donuk", "üşütücü", "kasvetli"],
    "sıcak": ["ılık", "hararetli", "kavurucu", "yakıcı", "kızgın"],
    "parlak": ["ışıldayan", "canlı", "göz alıcı", "ışıltılı", "parıltılı"],
    "sönük": ["mat", "cansız", "soluk", "donuk", "karanlık"],
    "açık": ["net", "belli", "şeffaf", "bariz", "ayrıntılı"],
    "kapalı": ["gizli", "örtük", "karanlık", "belirsiz", "bulanık"],
    "hata": ["yanlış", "kusur", "eksiklik", "yanılgı", "bozukluk"],
    "doğru": ["gerçek", "hakikat", "kesin", "dürüst", "yanılmaz"],
    "genç": ["taze", "toy", "dinamik", "delikanlı", "bebe"],
    "yaşlı": ["ihtiyar", "koca", "ak saçlı", "çınar", "tecrübeli"],
    "fakir": ["yoksul", "muhtaç", "zavallı", "gariban", "sefalet içinde"],
    "zengin": ["varlıklı", "paralı", "servet sahibi", "gösterişli", "müreffeh"],
    "aydınlık": ["parlak", "ışıklı", "berrak", "net", "göz kamaştırıcı"],
    "karanlık": ["loş", "sisli", "gizli", "kasvetli", "gölgelik"],
    "sert": ["katı", "haşin", "kaba", "ciddi", "katı kurallı"],
    "yumuşak": ["nazik", "ince", "hafif", "pürüzsüz", "duyarlı"],
    "yenilik": ["değişim", "gelişim", "modernleşme", "icraat", "inovasyon"],
    "gelenek": ["örf", "adet", "görenek", "kültür", "miras"],
    "hız": ["sürat", "ivme", "çabukluk", "şevk", "hızlanma"],
    "düşünce": ["fikir", "görüş", "kanaat", "akıl", "mantık"],
    "konuşma": ["söylem", "muhabbet", "sohbet", "ifade", "beşeri iletişim"],
    "yalan": ["doğru olmayan", "asılsız", "yanıltıcı", "hile", "gerçek dışı"],
    "gerçek": ["hakikat", "doğru", "mutlak", "kesin", "ispatlanmış"],
    "başarı": ["zafer", "kazanım", "üstünlük", "galibiyet", "beceri"],
    "başarısızlık": ["yenilgi", "kaybetme", "hedef şaşması", "bozgun", "hüsran"],
    "saygı": ["hürmet", "itibar", "taktir", "değer verme", "haysiyet"],
    "sevgi": ["aşk", "hoşlanma", "şefkat", "merhamet", "muhabbet"],
    "sabır": ["dayanıklılık", "tahammül", "direnç", "azim", "sükunet"],
    "öfke": ["sinir", "hiddet", "gazap", "kızgınlık", "hırçınlık"],
    "çalışkan": ["gayretli", "hırslı", "azimli", "üretken", "titiz"],
    "tembel": ["üşengeç", "ağırkanlı", "boşta gezen", "miskin", "gevşek"],
}


def synonym_replacement(text):
    words = text.split()
    new_words = []
    for word in words:
        if word in turkish_synonyms:
            new_words.append(random.choice(turkish_synonyms[word]))  # Replace with synonym
        else:
            new_words.append(word)
    return " ".join(new_words)

def augment_text(text):
    aug_texts = []
    aug_texts.append(synonym_replacement(text))  # Apply synonym replacement
    aug_texts.append(swap_aug.augment(text))  # Apply word swap
    aug_texts.append(del_aug.augment(text))  # Apply word deletion
    return aug_texts

# Apply augmentation while keeping labels
augmented_data = []
for _, row in train_df.iterrows():
    text, label = row["tweet"], row["label"]  # Extract tweet and label
    augmented_texts = augment_text(text)  # Generate 3 augmented samples
    for aug_text in augmented_texts:
        augmented_data.append({"tweet": aug_text[0], "label": label})  # Keep the same label

# Convert to DataFrame
aug_df = pd.DataFrame(augmented_data)

# Merge with original dataset
final_df = pd.concat([df, aug_df], ignore_index=True)

# Save the augmented dataset
final_df.to_csv("augmented_turkish_dataset.csv", index=False)

print("Turkish augmentation complete! Dataset size:", len(final_df))

test_df

final_df

from transformers import AutoTokenizer
from datasets import Dataset
from transformers import AutoModelForSequenceClassification, AutoModel


model_name = "akoksal/bounti"  # İstediğin modeli burada değiştir
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModel.from_pretrained(model_name)
# Dropout
model.config.hidden_dropout_prob = 0.5
model.config.attention_probs_dropout_prob = 0.5


def tokenize_function(examples):
    return tokenizer(examples["tweet"], padding="max_length", truncation=True)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(final_df)
test_dataset = Dataset.from_pandas(test_df)
# Tokenization
tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

#FINE - TUNE
import torch
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
import os
from transformers import EarlyStoppingCallback
os.environ["WANDB_DISABLED"] = "true"

num_labels = 3  # Pozitif, negatif, nötr
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)


training_args = TrainingArguments(
    output_dir="./bert_bounti_results",
    evaluation_strategy="epoch",  # Her epoch sonunda doğrulama yapılacak
    logging_strategy="epoch" , # Eğitim kaybını her epoch sonunda loglar
    save_strategy="epoch",        # En iyi modeli kaydetmek için
    load_best_model_at_end=True,  # En iyi modeli en sonda yükle
    per_device_train_batch_size=8,  # Küçük veri seti olduğu için batch size küçük olmalı
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    learning_rate=1e-5,
    logging_dir="./logs",

)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_test_dataset,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],  # 3 epoch boyunca gelişme olmazsa durdur

)


trainer.train()

import os
from datetime import datetime

# Modelin ismini veya modelin kendisini alabilirsiniz
model_name_short = model_name.split("/")[-1]

# Bugünün tarihini alalım
current_date = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

# Klasör ismi: model adı + tarih (örneğin: "bert-2025-04-06_14-30-00")
drive_save_path = f"/content/drive/MyDrive/fine_tuned_models/{model_name_short}_{current_date}"

# Klasör oluşturuluyor
os.makedirs(drive_save_path, exist_ok=True)

# Modeli ve tokenizer'ı kaydediyoruz
model.save_pretrained(drive_save_path)
tokenizer.save_pretrained(drive_save_path)

print(f"Model {drive_save_path} konumuna kaydedildi!")

import torch
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
import pandas as pd

# 1. Kaydedilen modeli ve tokenizer'ı yükleyin
model_name = "<model_adı>"  # Örnek: "bert-base-uncased" veya kendi model isminiz
drive_save_path = f"/content/drive/MyDrive/fine_tuned_models/bounti_2025-04-06_12-24-45"  # Model kaydettiğiniz klasör yolu

# Modeli ve tokenizer'ı yükleyin
model = AutoModelForSequenceClassification.from_pretrained(drive_save_path)
tokenizer = AutoTokenizer.from_pretrained(drive_save_path)

# 2. Test veri setini yükleyin (örneğin Hugging Face'den)
# Test veri setinizi farklı bir veri setiyle değiştirebilirsiniz
# Örnek: "glue", "imdb" veya kendi veri setiniz

# CSV dosyasını pandas DataFrame olarak yükleyin
test_df = pd.read_csv('/content/drive/MyDrive/random_50_labeled_tweets.csv')

# Pandas DataFrame'i Hugging Face Dataset formatına dönüştürün
test_dataset = load_dataset("pandas", data_files={'test': test_df})

# Test veri setinin sütunlarını kontrol edin ve doğru sütunları seçin
test_text_column = "text"  # Veri setinizdeki tweet textinin olduğu sütunun adı
test_label_column = "sentiment"  # Etiket sütunun adı

# 3. Etiketleri sayısal değerlere dönüştürme
# Örneğin, 'positive' 1, 'negative' 0 olacak şekilde
label_map = {"pozitif": 0, "negatif": 1, "nötr": 2}  # Etiketlerinizi buna göre ayarlayın
def map_labels(example):
    return {"sentiment": label_map.get(example[test_label_column], -1)}
test_dataset = test_dataset.map(map_labels)

# 4. Tokenizasyon
def tokenize_function(examples):
    return tokenizer(examples[test_text_column], padding="max_length", truncation=True, max_length=128)

tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

# 5. Test veri setini modelle değerlendirmek
# Verilerin PyTorch tensor'larına dönüştürülmesi
tokenized_test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "sentiment"])

# Modelin tahminlerde bulunması
model.eval()  # Modeli değerlendirme moduna al
predictions = []
labels = []

with torch.no_grad():
    for batch in tokenized_test_dataset['test']:
        inputs = {key: batch[key].unsqueeze(0).to(model.device) for key in ["input_ids", "attention_mask"]}
        label = batch["sentiment"].item()
        labels.append(label)

        outputs = model(**inputs)
        preds = torch.argmax(outputs.logits, dim=-1)
        predictions.append(preds.item())

# 6. Değerlendirme metrikleri
# Accuracy, f1-score ve diğer metrikler
accuracy = accuracy_score(labels, predictions)
print(f"Accuracy: {accuracy:.4f}")

# Classification report (F1, precision, recall)
print("Classification Report:")
print(classification_report(labels, predictions))

# Konfüzyon matrisi
cm = confusion_matrix(labels, predictions)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=list(label_map.keys()), yticklabels=list(label_map.keys()))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

test_df = pd.read_csv('/content/drive/MyDrive/random_50_labeled_tweets.csv')
print("Test DataFrame:")
print(test_df)
print(f"DataFrame boş mu? {test_df.empty}")

test_df = pd.read_csv('/content/drive/MyDrive/random_50_labeled_tweets.csv')
print("DataFrame Sütunları:")
print(test_df.columns)

import torch
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset, Dataset
import pandas as pd

# 1. Kaydedilen modeli ve tokenizer'ı yükleyin
model_name = "<model_adı>"  # Örnek: "bert-base-uncased" veya kendi model isminiz
drive_save_path = f"/content/drive/MyDrive/fine_tuned_models/bounti_2025-04-06_12-24-45"  # Model kaydettiğiniz klasör yolu

# Modeli ve tokenizer'ı yükleyin
model = AutoModelForSequenceClassification.from_pretrained(drive_save_path)
tokenizer = AutoTokenizer.from_pretrained(drive_save_path)

# 2. Test veri setini yükleyin
test_df = pd.read_csv('/content/drive/MyDrive/random_50_labeled_tweets.csv')

# Pandas DataFrame'i Hugging Face Dataset formatına dönüştürün
test_dataset = Dataset.from_pandas(test_df, split='test')

# Test veri setinin sütunlarını kontrol edin ve doğru sütunları seçin
test_text_column = "text"  # Test veri setindeki tweet textinin olduğu sütunun adı
test_label_column = "sentiment"  # Test veri setindeki etiket sütunun adı

# 3. Etiketleri sayısal değerlere dönüştürme (fine-tune edilen modelin etiketleriyle eşleşmeli)
label_map = {"Negatif": 1, "Pozitif": 0, "Nötr": 2}  # Fine-tune edilen modelin label_map'ine göre
def map_labels(example):
    return {"label": label_map.get(example[test_label_column])}
test_dataset = test_dataset.map(map_labels)

# 4. Tokenizasyon
def tokenize_function(examples):
    return tokenizer(examples[test_text_column], padding="max_length", truncation=True, max_length=128)

tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

# 5. Test veri setini modelle değerlendirmek
# Verilerin PyTorch tensor'larına dönüştürülmesi
tokenized_test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# Modelin tahminlerde bulunması
model.eval()  # Modeli değerlendirme moduna al
predictions = []
labels = []

with torch.no_grad():
    for batch in tokenized_test_dataset:
        inputs = {key: batch[key].unsqueeze(0).to(model.device) for key in ["input_ids", "attention_mask"]}
        label = batch["label"].item()
        labels.append(label)

        outputs = model(**inputs)
        preds = torch.argmax(outputs.logits, dim=-1)
        predictions.append(preds.item())

# 6. Değerlendirme metrikleri
# Accuracy, f1-score ve diğer metrikler
accuracy = accuracy_score(labels, predictions)
print(f"Accuracy: {accuracy:.4f}")

# Classification report (F1, precision, recall)
target_names = list(label_map.keys())
print("Classification Report:")
print(classification_report(labels, predictions, target_names=target_names))

# Konfüzyon matrisi
cm = confusion_matrix(labels, predictions)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=target_names, yticklabels=target_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

