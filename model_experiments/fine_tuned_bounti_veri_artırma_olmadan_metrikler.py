# -*- coding: utf-8 -*-
"""fine_tuned_bounti_veri_artırma_olmadan_metrikler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QMt6PdoKaVVSafqkri6XP721iWxK6TMl
"""

!pip install transformers datasets torch scikit-learn

import pandas as pd
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import Dataset, Value
from transformers import TrainingArguments, Trainer
from transformers.trainer_callback import EarlyStoppingCallback

# Veri setlerini yükle
train_df = pd.read_csv("/content/drive/MyDrive/30_data_every_each_class_labeled_tweets.csv")
test_df = pd.read_csv("/content/drive/MyDrive/random_50_labeled_tweets.csv")

# Etiketleri sayısal değere çevir
label_map = {"pozitif": 0, "negatif": 1, "nötr": 2, "Pozitif": 0, "Negatif": 1, "Nötr": 2}
train_df["label"] = train_df["label"].map(label_map)
train_df["label"] = train_df["label"].astype(int)

# Test veri setindeki etiket sütununu belirle ve dönüştür
if "sentiment" in test_df.columns:
    # NaN değerleri kontrol et ve varsayılan bir değerle doldur (örneğin: 0)
    test_df["label"] = test_df["sentiment"].map(label_map)
    # NaN değerleri 0 (pozitif) ile doldur - ihtiyaca göre değiştirilebilir
    test_df["label"] = test_df["label"].fillna(0).astype(int)
elif "label" in test_df.columns:
    test_df["label"] = test_df["label"].map(label_map)
    test_df["label"] = test_df["label"].fillna(0).astype(int)

# Tokenizer'ı yükle
model_name = "akoksal/bounti"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Veri setindeki metin sütunlarını belirle
train_text_column = "tweet" if "tweet" in train_df.columns else None
test_text_column = None
for col in ["tweet", "text", "content", "sentiment"]:
    if col in test_df.columns:
        test_text_column = col
        break

print(f"Eğitim metin sütunu: {train_text_column}")
print(f"Test metin sütunu: {test_text_column}")

# Tokenize fonksiyonunu düzenle - return_tensors parametresini kaldır
def tokenize_train(examples):
    # NaN değerler varsa boş string ile değiştir
    texts = examples[train_text_column]
    texts = [str(text) if text is not None else "" for text in texts]

    # Açık bir şekilde padding ve truncation parametrelerini belirt
    return tokenizer(
        texts,
        padding='max_length',  # Her örneği maksimum uzunluğa kadar doldur
        max_length=128,        # Maksimum uzunluğu belirt (modele göre ayarlayın)
        truncation=True,       # Çok uzun metinleri kısalt
    )

def tokenize_test(examples):
    # NaN değerler varsa boş string ile değiştir
    texts = examples[test_text_column]
    texts = [str(text) if text is not None else "" for text in texts]

    # Açık bir şekilde padding ve truncation parametrelerini belirt
    return tokenizer(
        texts,
        padding='max_length',  # Her örneği maksimum uzunluğa kadar doldur
        max_length=128,        # Maksimum uzunluğu belirt (modele göre ayarlayın)
        truncation=True,       # Çok uzun metinleri kısalt
    )

# Eğitim ve test için metin sütunlarını kontrol et
if train_text_column is None:
    raise ValueError("Eğitim veri setinde metin sütunu bulunamadı!")
if test_text_column is None:
    raise ValueError("Test veri setinde metin sütunu bulunamadı!")

# Örnek veriler yazdırılarak kontrol et
print("\nEğitim veri seti örneği:")
print(train_df[[train_text_column, "label"]].iloc[0])
print("\nTest veri seti örneği:")
print(test_df[[test_text_column, "sentiment"]].iloc[0])

# Etiketlerin doğru formatta olduğunu kontrol et ve NaN değerlerini raporla
print("\nEğitim etiketleri türü:", type(train_df["label"].iloc[0]))
print("Test etiketleri türü:", type(test_df["sentiment"].iloc[0]))
print("Test setinde NaN etiket sayısı:", test_df["label"].isna().sum())

# Veri setlerini Dataset formatına dönüştür
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenize işlemi
tokenized_train = train_dataset.map(tokenize_train, batched=True, remove_columns=train_dataset.column_names)
tokenized_test = test_dataset.map(tokenize_test, batched=True, remove_columns=test_dataset.column_names)

# Etiketleri ekle - test etiketlerinin NaN değerler içermediğinden emin ol
tokenized_train = tokenized_train.add_column("labels", train_df["label"].tolist())
# NaN olmayan, sayısal etiketler ekle
tokenized_test = tokenized_test.add_column("labels", test_df["label"].tolist())

# İlk birkaç veriyi kontrol et
print("\nTokenize edilmiş eğitim verisi örneği:")
print(tokenized_train[0])
print("\nTokenize edilmiş test verisi örneği:")
print(tokenized_test[0])
print("\nTest labels içinde NaN var mı?", np.isnan(np.array(test_df["label"].tolist())).any())

# Modeli yükle
num_labels = 3
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

# Etiketleri int64 tipine dönüştür
tokenized_train = tokenized_train.cast_column("labels", Value("int64"))
tokenized_test = tokenized_test.cast_column("labels", Value("int64"))

training_args = TrainingArguments(
    output_dir="./bert_bounti_results",
    evaluation_strategy="epoch",  # Her epoch sonunda doğrulama yapılacak
    logging_strategy="epoch" , # Eğitim kaybını her epoch sonunda loglar
    save_strategy="epoch",        # En iyi modeli kaydetmek için
    load_best_model_at_end=True,  # En iyi modeli en sonda yükle
    per_device_train_batch_size=8,  # Küçük veri seti olduğu için batch size küçük olmalı
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    learning_rate=1e-5,
    logging_dir="./logs",

)

from transformers import set_seed
set_seed(42)

# ✅ 7. Trainer ile fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)
trainer.train()

import os
model_save_path = "/content/drive/MyDrive/fine_tuned_models/bounti_finetuned_veri_artirmasiz"
os.makedirs(model_save_path, exist_ok=True)
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"✅ Model başarıyla kaydedildi: {model_save_path}")

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ✅ 3. Tahmin yap
predictions = trainer.predict(tokenized_test)
preds = np.argmax(predictions.predictions, axis=1)

# 🎯 DÜZELTİLDİ: string etiketler yerine sayısal olanı alıyoruz
true_labels = test_df["label"].values

# ✅ 4. Raporlama
print("\n📊 Classification Report:\n")
print(classification_report(true_labels, preds, target_names=["pozitif", "negatif", "nötr"]))

acc = accuracy_score(true_labels, preds)
print(f"✅ Accuracy: {acc:.4f}")

# ✅ 5. Confusion Matrix
reverse_label_map = {0: "pozitif", 1: "negatif", 2: "nötr"}  # Eğer yukarıda tanımlı değilse
cm = confusion_matrix(true_labels, preds)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=reverse_label_map.values(), yticklabels=reverse_label_map.values(), cmap="Blues")
plt.xlabel("Tahmin")
plt.ylabel("Gerçek")
plt.title("📌 Confusion Matrix")
plt.show()

