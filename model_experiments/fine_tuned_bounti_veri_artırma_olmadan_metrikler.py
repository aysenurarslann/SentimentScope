# -*- coding: utf-8 -*-
"""fine_tuned_bounti_veri_artÄ±rma_olmadan_metrikler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QMt6PdoKaVVSafqkri6XP721iWxK6TMl
"""

!pip install transformers datasets torch scikit-learn

import pandas as pd
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import Dataset, Value
from transformers import TrainingArguments, Trainer
from transformers.trainer_callback import EarlyStoppingCallback

# Veri setlerini yÃ¼kle
train_df = pd.read_csv("/content/drive/MyDrive/30_data_every_each_class_labeled_tweets.csv")
test_df = pd.read_csv("/content/drive/MyDrive/random_50_labeled_tweets.csv")

# Etiketleri sayÄ±sal deÄŸere Ã§evir
label_map = {"pozitif": 0, "negatif": 1, "nÃ¶tr": 2, "Pozitif": 0, "Negatif": 1, "NÃ¶tr": 2}
train_df["label"] = train_df["label"].map(label_map)
train_df["label"] = train_df["label"].astype(int)

# Test veri setindeki etiket sÃ¼tununu belirle ve dÃ¶nÃ¼ÅŸtÃ¼r
if "sentiment" in test_df.columns:
    # NaN deÄŸerleri kontrol et ve varsayÄ±lan bir deÄŸerle doldur (Ã¶rneÄŸin: 0)
    test_df["label"] = test_df["sentiment"].map(label_map)
    # NaN deÄŸerleri 0 (pozitif) ile doldur - ihtiyaca gÃ¶re deÄŸiÅŸtirilebilir
    test_df["label"] = test_df["label"].fillna(0).astype(int)
elif "label" in test_df.columns:
    test_df["label"] = test_df["label"].map(label_map)
    test_df["label"] = test_df["label"].fillna(0).astype(int)

# Tokenizer'Ä± yÃ¼kle
model_name = "akoksal/bounti"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Veri setindeki metin sÃ¼tunlarÄ±nÄ± belirle
train_text_column = "tweet" if "tweet" in train_df.columns else None
test_text_column = None
for col in ["tweet", "text", "content", "sentiment"]:
    if col in test_df.columns:
        test_text_column = col
        break

print(f"EÄŸitim metin sÃ¼tunu: {train_text_column}")
print(f"Test metin sÃ¼tunu: {test_text_column}")

# Tokenize fonksiyonunu dÃ¼zenle - return_tensors parametresini kaldÄ±r
def tokenize_train(examples):
    # NaN deÄŸerler varsa boÅŸ string ile deÄŸiÅŸtir
    texts = examples[train_text_column]
    texts = [str(text) if text is not None else "" for text in texts]

    # AÃ§Ä±k bir ÅŸekilde padding ve truncation parametrelerini belirt
    return tokenizer(
        texts,
        padding='max_length',  # Her Ã¶rneÄŸi maksimum uzunluÄŸa kadar doldur
        max_length=128,        # Maksimum uzunluÄŸu belirt (modele gÃ¶re ayarlayÄ±n)
        truncation=True,       # Ã‡ok uzun metinleri kÄ±salt
    )

def tokenize_test(examples):
    # NaN deÄŸerler varsa boÅŸ string ile deÄŸiÅŸtir
    texts = examples[test_text_column]
    texts = [str(text) if text is not None else "" for text in texts]

    # AÃ§Ä±k bir ÅŸekilde padding ve truncation parametrelerini belirt
    return tokenizer(
        texts,
        padding='max_length',  # Her Ã¶rneÄŸi maksimum uzunluÄŸa kadar doldur
        max_length=128,        # Maksimum uzunluÄŸu belirt (modele gÃ¶re ayarlayÄ±n)
        truncation=True,       # Ã‡ok uzun metinleri kÄ±salt
    )

# EÄŸitim ve test iÃ§in metin sÃ¼tunlarÄ±nÄ± kontrol et
if train_text_column is None:
    raise ValueError("EÄŸitim veri setinde metin sÃ¼tunu bulunamadÄ±!")
if test_text_column is None:
    raise ValueError("Test veri setinde metin sÃ¼tunu bulunamadÄ±!")

# Ã–rnek veriler yazdÄ±rÄ±larak kontrol et
print("\nEÄŸitim veri seti Ã¶rneÄŸi:")
print(train_df[[train_text_column, "label"]].iloc[0])
print("\nTest veri seti Ã¶rneÄŸi:")
print(test_df[[test_text_column, "sentiment"]].iloc[0])

# Etiketlerin doÄŸru formatta olduÄŸunu kontrol et ve NaN deÄŸerlerini raporla
print("\nEÄŸitim etiketleri tÃ¼rÃ¼:", type(train_df["label"].iloc[0]))
print("Test etiketleri tÃ¼rÃ¼:", type(test_df["sentiment"].iloc[0]))
print("Test setinde NaN etiket sayÄ±sÄ±:", test_df["label"].isna().sum())

# Veri setlerini Dataset formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenize iÅŸlemi
tokenized_train = train_dataset.map(tokenize_train, batched=True, remove_columns=train_dataset.column_names)
tokenized_test = test_dataset.map(tokenize_test, batched=True, remove_columns=test_dataset.column_names)

# Etiketleri ekle - test etiketlerinin NaN deÄŸerler iÃ§ermediÄŸinden emin ol
tokenized_train = tokenized_train.add_column("labels", train_df["label"].tolist())
# NaN olmayan, sayÄ±sal etiketler ekle
tokenized_test = tokenized_test.add_column("labels", test_df["label"].tolist())

# Ä°lk birkaÃ§ veriyi kontrol et
print("\nTokenize edilmiÅŸ eÄŸitim verisi Ã¶rneÄŸi:")
print(tokenized_train[0])
print("\nTokenize edilmiÅŸ test verisi Ã¶rneÄŸi:")
print(tokenized_test[0])
print("\nTest labels iÃ§inde NaN var mÄ±?", np.isnan(np.array(test_df["label"].tolist())).any())

# Modeli yÃ¼kle
num_labels = 3
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

# Etiketleri int64 tipine dÃ¶nÃ¼ÅŸtÃ¼r
tokenized_train = tokenized_train.cast_column("labels", Value("int64"))
tokenized_test = tokenized_test.cast_column("labels", Value("int64"))

training_args = TrainingArguments(
    output_dir="./bert_bounti_results",
    evaluation_strategy="epoch",  # Her epoch sonunda doÄŸrulama yapÄ±lacak
    logging_strategy="epoch" , # EÄŸitim kaybÄ±nÄ± her epoch sonunda loglar
    save_strategy="epoch",        # En iyi modeli kaydetmek iÃ§in
    load_best_model_at_end=True,  # En iyi modeli en sonda yÃ¼kle
    per_device_train_batch_size=8,  # KÃ¼Ã§Ã¼k veri seti olduÄŸu iÃ§in batch size kÃ¼Ã§Ã¼k olmalÄ±
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    learning_rate=1e-5,
    logging_dir="./logs",

)

from transformers import set_seed
set_seed(42)

# âœ… 7. Trainer ile fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)
trainer.train()

import os
model_save_path = "/content/drive/MyDrive/fine_tuned_models/bounti_finetuned_veri_artirmasiz"
os.makedirs(model_save_path, exist_ok=True)
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"âœ… Model baÅŸarÄ±yla kaydedildi: {model_save_path}")

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# âœ… 3. Tahmin yap
predictions = trainer.predict(tokenized_test)
preds = np.argmax(predictions.predictions, axis=1)

# ğŸ¯ DÃœZELTÄ°LDÄ°: string etiketler yerine sayÄ±sal olanÄ± alÄ±yoruz
true_labels = test_df["label"].values

# âœ… 4. Raporlama
print("\nğŸ“Š Classification Report:\n")
print(classification_report(true_labels, preds, target_names=["pozitif", "negatif", "nÃ¶tr"]))

acc = accuracy_score(true_labels, preds)
print(f"âœ… Accuracy: {acc:.4f}")

# âœ… 5. Confusion Matrix
reverse_label_map = {0: "pozitif", 1: "negatif", 2: "nÃ¶tr"}  # EÄŸer yukarÄ±da tanÄ±mlÄ± deÄŸilse
cm = confusion_matrix(true_labels, preds)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=reverse_label_map.values(), yticklabels=reverse_label_map.values(), cmap="Blues")
plt.xlabel("Tahmin")
plt.ylabel("GerÃ§ek")
plt.title("ğŸ“Œ Confusion Matrix")
plt.show()

